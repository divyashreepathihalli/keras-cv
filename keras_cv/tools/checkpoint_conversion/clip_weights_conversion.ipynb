{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DhV6hzOMY0W"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRzYR-oFgxt1",
    "outputId": "e4b01fcd-9f71-4ba7-b8a2-1796f7ef260d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for keras-cv (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras==3.0.2\n",
      "  Downloading keras-3.0.2-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras==3.0.2) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras==3.0.2) (1.23.5)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras==3.0.2) (13.7.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras==3.0.2) (0.0.7)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras==3.0.2) (3.9.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras==3.0.2) (0.1.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.0.2) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.0.2) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.0.2) (0.1.2)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/divyashreepathihalli/keras-cv.git@CLIP_refactor\n",
    "!pip install -q keras-nlp\n",
    "!pip install -q tf-keras\n",
    "!pip install -q tensorflow-text\n",
    "!pip install keras==3.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdGT8Em4Mc4b"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDvJmQuug4-x"
   },
   "outputs": [],
   "source": [
    "from keras_cv.models.feature_extractor.clip import CLIPProcessor\n",
    "import keras\n",
    "from keras_cv.models import CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuFgha2jTshi",
    "outputId": "b99d73eb-cc97-47d0-f46e-687c9e8b8237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-02 22:19:20--  https://i.imgur.com/8H7XCH0.jpg\n",
      "Resolving i.imgur.com (i.imgur.com)... 151.101.52.193\n",
      "Connecting to i.imgur.com (i.imgur.com)|151.101.52.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44544 (44K) [image/jpeg]\n",
      "Saving to: ‘cat.jpg’\n",
      "\n",
      "\rcat.jpg               0%[                    ]       0  --.-KB/s               \rcat.jpg             100%[===================>]  43.50K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-02-02 22:19:20 (3.58 MB/s) - ‘cat.jpg’ saved [44544/44544]\n",
      "\n",
      "--2024-02-02 22:19:20--  http://images.cocodataset.org/val2017/000000039769.jpg\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.78.4, 3.5.1.13, 52.217.139.73, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.78.4|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 173131 (169K) [image/jpeg]\n",
      "Saving to: ‘test.jpg’\n",
      "\n",
      "test.jpg            100%[===================>] 169.07K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-02-02 22:19:20 (2.67 MB/s) - ‘test.jpg’ saved [173131/173131]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://i.imgur.com/8H7XCH0.jpg -O cat.jpg\n",
    "!wget http://images.cocodataset.org/val2017/000000039769.jpg -O test.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "X3kkmK6h_gFH"
   },
   "outputs": [],
   "source": [
    "# @title Select which model weights you would like to convert\n",
    "MODEL_CONFIGS = {\n",
    "    \"CLIP_B32\": {\n",
    "        \"embed_dim\": 512,\n",
    "        \"context_length\": 77,\n",
    "        \"vocab_size\": 49408,\n",
    "        \"transformer_width\": 512,\n",
    "        \"transformer_heads\": 8,\n",
    "        \"transformer_layers\": 12,\n",
    "        \"vision_layers\": 12,\n",
    "        \"vision_width\": 768,\n",
    "        \"image_resolution\": 224,\n",
    "        \"vision_patch_size\": 32,\n",
    "    },\n",
    "    \"CLIP_B16\": {\n",
    "        \"embed_dim\": 512,\n",
    "        \"context_length\": 77,\n",
    "        \"vocab_size\": 49408,\n",
    "        \"transformer_width\": 512,\n",
    "        \"transformer_heads\": 8,\n",
    "        \"transformer_layers\": 12,\n",
    "        \"vision_layers\": 12,\n",
    "        \"vision_width\": 768,\n",
    "        \"image_resolution\": 224,\n",
    "        \"vision_patch_size\": 16,\n",
    "    },\n",
    "    \"CLIP_L14\": {\n",
    "        \"embed_dim\": 768,\n",
    "        \"context_length\": 77,\n",
    "        \"vocab_size\": 49408,\n",
    "        \"transformer_width\": 768,\n",
    "        \"transformer_heads\": 12,\n",
    "        \"transformer_layers\": 12,\n",
    "        \"vision_layers\": 24,\n",
    "        \"vision_width\": 1024,\n",
    "        \"image_resolution\": 224,\n",
    "        \"vision_patch_size\": 14,\n",
    "    },\n",
    "    \"CLIP_L14_336\": {\n",
    "        \"embed_dim\": 768,\n",
    "        \"context_length\": 77,\n",
    "        \"vocab_size\": 49408,\n",
    "        \"transformer_width\": 768,\n",
    "        \"transformer_heads\": 12,\n",
    "        \"transformer_layers\": 12,\n",
    "        \"vision_layers\": 24,\n",
    "        \"vision_width\": 1024,\n",
    "        \"image_resolution\": 336,\n",
    "        \"vision_patch_size\": 14,\n",
    "    },\n",
    "}\n",
    "model_map_hf = {\n",
    "    \"CLIP_B16\": \"openai/clip-vit-base-patch32\",\n",
    "    \"CLIP_B32\": \"openai/clip-vit-base-patch16\",\n",
    "    \"CLIP_L14\": \"openai/clip-vit-large-patch14\",\n",
    "    \"CLIP_L14_336\": \"openai/clip-vit-large-patch14-336\",\n",
    "}\n",
    "config_name = \"CLIP_L14_336\"  # @param [\"CLIP_B16\", \"CLIP_B32\", \"CLIP_L14\", \"CLIP_L14_336\"]\n",
    "config_name_hf = model_map_hf[config_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l3Ll7dMMd-m"
   },
   "source": [
    "# Keras 3 CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urhuhwq0Dczo"
   },
   "outputs": [],
   "source": [
    "embed_dim = MODEL_CONFIGS[config_name][\"embed_dim\"]\n",
    "context_length = MODEL_CONFIGS[config_name][\"context_length\"]\n",
    "vocab_size = MODEL_CONFIGS[config_name][\"vocab_size\"]\n",
    "transformer_width = MODEL_CONFIGS[config_name][\"transformer_width\"]\n",
    "transformer_heads = MODEL_CONFIGS[config_name][\"transformer_heads\"]\n",
    "transformer_layers = MODEL_CONFIGS[config_name][\"transformer_layers\"]\n",
    "vision_layers = MODEL_CONFIGS[config_name][\"vision_layers\"]\n",
    "vision_width = MODEL_CONFIGS[config_name][\"vision_width\"]\n",
    "vision_patch_size = MODEL_CONFIGS[config_name][\"vision_patch_size\"]\n",
    "image_resolution = MODEL_CONFIGS[config_name][\"image_resolution\"]\n",
    "model = CLIP(\n",
    "    embed_dim,\n",
    "    image_resolution,\n",
    "    vision_layers,\n",
    "    vision_width,\n",
    "    vision_patch_size,\n",
    "    context_length,\n",
    "    vocab_size,\n",
    "    transformer_width,\n",
    "    transformer_heads,\n",
    "    transformer_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "uE6x7gfqa3Ee",
    "outputId": "9a080569-7ab9-49ad-8589-87f335ef2f31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"clip\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"clip\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                       </span>┃<span style=\"font-weight: bold\"> Output Shape                  </span>┃<span style=\"font-weight: bold\">     Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│ image_encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CLIPImageEncoder</span>)   │ ?                             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
       "│ text_encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CLIPTextEncoder</span>)     │ ?                             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└────────────────────────────────────┴───────────────────────────────┴─────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│ image_encoder (\u001b[38;5;33mCLIPImageEncoder\u001b[0m)   │ ?                             │ \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
       "│ text_encoder (\u001b[38;5;33mCLIPTextEncoder\u001b[0m)     │ ?                             │ \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└────────────────────────────────────┴───────────────────────────────┴─────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,425</span> (154.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m39,425\u001b[0m (154.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,425</span> (154.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m39,425\u001b[0m (154.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buXKlNfGTenW"
   },
   "outputs": [],
   "source": [
    "processor = CLIPProcessor(224, \"vocab.json\", \"merges.txt\")\n",
    "image = processor.process_images([\"cat.jpg\"])\n",
    "text_input = [\n",
    "    \"photo of a cat on a tortoise\",\n",
    "    \"tortoise on a dog\",\n",
    "    \"a photo of a tortoise\",\n",
    "]\n",
    "text = processor.process_texts(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHSpMv0PT5SX",
    "outputId": "566c92c4-fbf3-4e2d-87f1-6112b2cff96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 0.42190465  0.6262117  -0.2368357 ]], shape=(1, 3), dtype=float32)\n",
      "tortoise on a dog\n"
     ]
    }
   ],
   "source": [
    "image_logits, text_logits = model(image, text)\n",
    "output = keras.layers.Softmax()(image_logits)\n",
    "print(image_logits)\n",
    "print(text_input[keras.ops.argmax(output)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "GgNBvYCTtmA3",
    "outputId": "35b9a26c-325e-4535-c33b-3f67ab112e19"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"clip\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"clip\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                       </span>┃<span style=\"font-weight: bold\"> Output Shape                  </span>┃<span style=\"font-weight: bold\">     Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│ image_encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CLIPImageEncoder</span>)   │ ?                             │  <span style=\"color: #00af00; text-decoration-color: #00af00\">87,849,216</span> │\n",
       "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
       "│ text_encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CLIPTextEncoder</span>)     │ ?                             │  <span style=\"color: #00af00; text-decoration-color: #00af00\">63,428,096</span> │\n",
       "└────────────────────────────────────┴───────────────────────────────┴─────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│ image_encoder (\u001b[38;5;33mCLIPImageEncoder\u001b[0m)   │ ?                             │  \u001b[38;5;34m87,849,216\u001b[0m │\n",
       "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
       "│ text_encoder (\u001b[38;5;33mCLIPTextEncoder\u001b[0m)     │ ?                             │  \u001b[38;5;34m63,428,096\u001b[0m │\n",
       "└────────────────────────────────────┴───────────────────────────────┴─────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">151,277,313</span> (577.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m151,277,313\u001b[0m (577.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">151,277,313</span> (577.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m151,277,313\u001b[0m (577.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8DWYq_hVFnz"
   },
   "source": [
    "# HF CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3W2prd6C0pxe"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor as CP\n",
    "from transformers import CLIPModel as CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EntuvOq1MhwU",
    "outputId": "e154a367-2f94-4fa1-e97d-d2f32db7a2cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "model_hf = CM.from_pretrained(config_name_hf)\n",
    "processor = CP.from_pretrained(config_name_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ep8DRTkv3AwS",
    "outputId": "770756bc-8829-484f-b6e5-763fe81e24d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9957, 0.0023, 0.0020]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://i.imgur.com/8H7XCH0.jpg\"\n",
    "image_hf = Image.open(requests.get(url, stream=True).raw)\n",
    "text_inputs = [\n",
    "    \"photo of a cat on a tortoise\",\n",
    "    \"tortoise on a dog\",\n",
    "    \"a photo of a tortoise\",\n",
    "]\n",
    "inputs = processor(\n",
    "    text=text_inputs, images=image_hf, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "\n",
    "outputs = model_hf(**inputs)\n",
    "logits_per_image = (\n",
    "    outputs.logits_per_image\n",
    ")  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(\n",
    "    dim=1\n",
    ")  # we can take the softmax to get the label probabilitiesprobs\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPa0cVnY3cBC"
   },
   "outputs": [],
   "source": [
    "# hugging face weights\n",
    "hf_wts = model_hf.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArkCHlVZVKfM"
   },
   "source": [
    "# Copy weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUCpKltRG4Gd"
   },
   "source": [
    "##vision encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn_U02N7U2VN"
   },
   "outputs": [],
   "source": [
    "model.logit_scale.assign(hf_wts.pop(\"logit_scale\").numpy())\n",
    "model.get_layer(\"image_encoder\").get_layer(\n",
    "    \"clip_patching_and_embedding\"\n",
    ").class_embedding.assign(\n",
    "    hf_wts.pop(\"vision_model.embeddings.class_embedding\").numpy()\n",
    ")\n",
    "model.get_layer(\"image_encoder\").get_layer(\n",
    "    \"clip_patching_and_embedding\"\n",
    ").positional_embedding.assign(\n",
    "    hf_wts.pop(\"vision_model.embeddings.position_embedding.weight\").numpy()\n",
    ")\n",
    "model.get_layer(\"image_encoder\").get_layer(\n",
    "    \"clip_patching_and_embedding\"\n",
    ").conv1.weights[0].assign(\n",
    "    hf_wts.pop(\"vision_model.embeddings.patch_embedding.weight\")\n",
    "    .permute(3, 2, 1, 0)\n",
    "    .numpy()\n",
    ")\n",
    "model.get_layer(\"image_encoder\").get_layer(\"ln_1\").weights[0].assign(\n",
    "    hf_wts.pop(\"vision_model.pre_layrnorm.weight\").numpy()\n",
    ")\n",
    "model.get_layer(\"image_encoder\").get_layer(\"ln_1\").weights[1].assign(\n",
    "    hf_wts.pop(\"vision_model.pre_layrnorm.bias\").numpy()\n",
    ")\n",
    "model.get_layer(\"image_encoder\").get_layer(\"ln_2\").weights[0].assign(\n",
    "    hf_wts.pop(\"vision_model.post_layernorm.weight\").numpy()\n",
    ")\n",
    "model.get_layer(\"image_encoder\").get_layer(\"ln_2\").weights[1].assign(\n",
    "    hf_wts.pop(\"vision_model.post_layernorm.bias\").numpy()\n",
    ")\n",
    "model.get_layer(\"image_encoder\").get_layer(\"vision_projector\").weights[\n",
    "    0\n",
    "].assign(hf_wts.pop(\"visual_projection.weight\").transpose(1, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qptfuWobZcbT"
   },
   "outputs": [],
   "source": [
    "for i in range(0, MODEL_CONFIGS[config_name][\"vision_layers\"]):\n",
    "    if i == 0:\n",
    "        residual_attention = f\"residual_attention\"\n",
    "    else:\n",
    "        residual_attention = f\"residual_attention_{i}\"\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).attn.q_proj.weights[0].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.self_attn.q_proj.weight\")\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).attn.q_proj.weights[1].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.self_attn.q_proj.bias\")\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).attn.k_proj.weights[0].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.self_attn.k_proj.weight\")\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).attn.k_proj.weights[1].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.self_attn.k_proj.bias\")\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).attn.v_proj.weights[0].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.self_attn.v_proj.weight\")\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).attn.v_proj.weights[1].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.self_attn.v_proj.bias\")\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).attn.out_proj.weights[1].assign(\n",
    "        hf_wts.pop(\n",
    "            f\"vision_model.encoder.layers.{i}.self_attn.out_proj.bias\"\n",
    "        ).numpy()\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).attn.out_proj.weights[0].assign(\n",
    "        hf_wts.pop(\n",
    "            f\"vision_model.encoder.layers.{i}.self_attn.out_proj.weight\"\n",
    "        ).numpy()\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).ln_1.weights[0].assign(\n",
    "        hf_wts.pop(\n",
    "            f\"vision_model.encoder.layers.{i}.layer_norm1.weight\"\n",
    "        ).numpy()\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).ln_1.weights[1].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.layer_norm1.bias\").numpy()\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).ln_2.weights[0].assign(\n",
    "        hf_wts.pop(\n",
    "            f\"vision_model.encoder.layers.{i}.layer_norm2.weight\"\n",
    "        ).numpy()\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).ln_2.weights[1].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.layer_norm2.bias\").numpy()\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).mlp.get_layer(\"c_fc\").weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.mlp.fc1.weight\")\n",
    "        .transpose(1, 0)\n",
    "        .numpy()\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).mlp.get_layer(\"c_fc\").weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.mlp.fc1.bias\").numpy()\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).mlp.get_layer(\"c_proj\").weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.mlp.fc2.weight\")\n",
    "        .transpose(1, 0)\n",
    "        .numpy()\n",
    "    )\n",
    "    model.get_layer(\"image_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(residual_attention).mlp.get_layer(\"c_proj\").weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"vision_model.encoder.layers.{i}.mlp.fc2.bias\").numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RN2aVrYG8T3"
   },
   "source": [
    "## Text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FtDROnynb0N"
   },
   "outputs": [],
   "source": [
    "num_transformer_layers = MODEL_CONFIGS[config_name][\"vision_layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1AD7TcbdWEC"
   },
   "outputs": [],
   "source": [
    "model.get_layer(\"text_encoder\").get_layer(\"text_projector\").weights[0].assign(\n",
    "    hf_wts.pop(\"text_projection.weight\").numpy()\n",
    ")\n",
    "model.get_layer(\"text_encoder\").get_layer(\"token_embedding\").weights[0].assign(\n",
    "    hf_wts.pop(\"text_model.embeddings.token_embedding.weight\").numpy()\n",
    ")\n",
    "model.get_layer(\"text_encoder\").positional_embedding.assign(\n",
    "    hf_wts.pop(\"text_model.embeddings.position_embedding.weight\").numpy()\n",
    ")\n",
    "model.get_layer(\"text_encoder\").get_layer(\"ln_final\").weights[0].assign(\n",
    "    hf_wts.pop(\"text_model.final_layer_norm.weight\")\n",
    ")\n",
    "model.get_layer(\"text_encoder\").get_layer(\"ln_final\").weights[1].assign(\n",
    "    hf_wts.pop(\"text_model.final_layer_norm.bias\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6leOiFO6V2U"
   },
   "outputs": [],
   "source": [
    "for i in range(MODEL_CONFIGS[config_name][\"transformer_layers\"]):\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).attn.k_proj.weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.self_attn.k_proj.weight\")\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).attn.k_proj.weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.self_attn.k_proj.bias\")\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).attn.q_proj.weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.self_attn.q_proj.weight\")\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).attn.q_proj.weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.self_attn.q_proj.bias\")\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).attn.v_proj.weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.self_attn.v_proj.weight\")\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).attn.v_proj.weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.self_attn.v_proj.bias\")\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).attn.out_proj.weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.self_attn.out_proj.weight\")\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).attn.out_proj.weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.self_attn.out_proj.bias\")\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).ln_1.weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.layer_norm1.weight\").numpy()\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).ln_1.weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.layer_norm1.bias\").numpy()\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).ln_2.weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.layer_norm2.weight\").numpy()\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).ln_2.weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.layer_norm2.bias\").numpy()\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).mlp.get_layer(\n",
    "        \"c_fc\"\n",
    "    ).weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.mlp.fc1.weight\")\n",
    "        .transpose(1, 0)\n",
    "        .numpy()\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).mlp.get_layer(\n",
    "        \"c_fc\"\n",
    "    ).weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.mlp.fc1.bias\").numpy()\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).mlp.get_layer(\n",
    "        \"c_proj\"\n",
    "    ).weights[\n",
    "        0\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.mlp.fc2.weight\")\n",
    "        .transpose(1, 0)\n",
    "        .numpy()\n",
    "    )\n",
    "    model.get_layer(\"text_encoder\").get_layer(\n",
    "        \"clip_encoder\"\n",
    "    ).resblocks.get_layer(\n",
    "        f\"residual_attention_{num_transformer_layers+i}\"\n",
    "    ).mlp.get_layer(\n",
    "        \"c_proj\"\n",
    "    ).weights[\n",
    "        1\n",
    "    ].assign(\n",
    "        hf_wts.pop(f\"text_model.encoder.layers.{i}.mlp.fc2.bias\").numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bgen7hxCCeZ7",
    "outputId": "c777d6f1-4aa7-4f3e-8fd7-759364364c44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that we copied all weights\n",
    "hf_wts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlfDdO-mid62"
   },
   "source": [
    "# save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QscCUUZFiqBV"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"clip-vit-base-patch32.weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
